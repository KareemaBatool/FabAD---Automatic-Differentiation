{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "\n",
    "# fab_ad Documentation\n",
    "\n",
    "December 9, 2022\n",
    "\n",
    "Team: Kareema Batool, Nikhil Nayak, Nishtha Sardana, Saket Joshi, Sree Harsha Tanneru\n",
    "Note: Our badge is not working because of some technical glitch (on Github's side) and we have already discussed this with our assigned TF\n",
    "## 1. Introduction\n",
    "\n",
    "Differentiation is defined as the process of finding the gradients/derivatives of a particular function in hand. It finds multiple applications in the areas of science and engineering. With the exponential growth in the size of the dataset and advancements in technologies - the complexity of computing derivatives has increased and we have become increasingly dependent on computers to compute derivatives. \n",
    "\n",
    "Currently, there are three ways to compute derivatives - finite, symbolic, automatic differentiation. The finite differentiation method although being quick and easy to implement - suffers from machine precision and rounding errors. We are able to alleviate these issues using symbolic differentiation, however, it becomes computationally very expensive as the function(s) starts to get complex. We are able to alleviate both the issues of computational complexity and machine precision using automatic differentiation.  \n",
    "\n",
    "Automatic Differentiation leverages symbolic rules for evaluating gradients - which is more accurate than using finite difference approximations. But unlike a purely symbolic process, the evaluation of expressions takes place early in the computation - it evaluates derivatives at particular numeric values. \n",
    "\n",
    "The package `fab-ad` implements automatic differentiation for computational use. `fab-ad` can be used to automatically differentiate functions via forward mode. Automatic Differentiation finds applications in optimization, machine learning, and numerical methods. \n",
    "\n",
    "## 2. Background\n",
    "### 2.1 An overview of Automatic Differentiation\n",
    "\n",
    "Automatic differentiation (also known as autodiff, AD, or algorithmic differentiation) is a widely used tool in optimization. There are two ways in which we can calculate the derivative of a function. If it is  totally new function, we can use the limit definition to derive it. However, most of the time  a function is just a composition of old (seen) functions and in this case we can use chain rule to derive the new function. \n",
    "\n",
    "Automatic differentiation provides the benefit of avoiding the symbolic manipulation of functions while achieving machine-like precision. Automatic differentiation has applications in astronomy, dynamic systems, numerical analytical research, optimization in finance and engineering, among others.\n",
    "\n",
    "AD is based on the concept of decomposing a function into a series of simple operations and functions whose derivatives are readily attainable, and then sequentially applying the chain rule to assess the derivatives of these operations to calculate the derivative of the entire function. This eliminates the difficulty of parsing and memorizing the entire symbolic expression, allowing us to keep simply the function's value and its derivative at each step.\n",
    "\n",
    "The two primary strategies for automated distinction are forward mode and reverse mode. These two modes are identical in terms of precision, but they may differ in terms of efficiency when the size of the input data grows. Forward mode is often more effective when working with complicated functions or a large number of functions. Some AD algorithms even implement both forward mode and reverse mode. For this project, our package implements solely the forward mode and is a valuable resource for applications such as dynamic systems and mechanical engineering.\n",
    "\n",
    "To better comprehend automated differentiation, let's first familiarize ourselves with several essential principles employed in the AD algorithms. We will use the remainder of this section to introduce them quickly.\n",
    "\n",
    "### 2.2 Elementary operations and functions\n",
    "\n",
    "Automatic differentiation's algorithm reduces functions to simple arithmetic operations and elementary functions. The fundamental arithmetic operations are addition, subtraction, multiplication, and division (we can also consider taking roots of a number as raising it to powers less than ). Among the elementary functions are exponential, logarithmic, and trigonometric. These operations and functions have easily calculable derivates, thus we employ them as elementary evaluation stages in the AD evaluation trace.\n",
    "\n",
    "### 2.3 The chain rule\n",
    "\n",
    "The chain rule permits the decomposition of complex, nested functions into layers of operations. Our automated differentiation algorithm sequentially computes the derivative of functions using the chain rule.\n",
    "\n",
    "The chain rule can be used to calculate the derivate of nested functions, such in the form of $u(v(t))$. For this function, the derivative of $u$ with respect to $t$ is $$\\dfrac{\\partial u}{\\partial t} = \\dfrac{\\partial u}{\\partial v}\\dfrac{\\partial v}{\\partial t}.$$\n",
    "\n",
    "A more general form of chain rule applies when a function $h$ has several arguments, or when its argument is a vector. Suppose we have $h = h(y(t))$ where  $y \\in R^n$ and $t \\in R^m $. Here, $h$ is the combination of $n$ functions, each of which has $m$ variables. Using the chain rule, the derivative of $h$ with respect to $t$, now called the gradient of $h$, is\n",
    "\n",
    "$$\\nabla_{t}h = \\sum_{i=1}^{n}{\\frac{\\partial h}{\\partial y_{i}}\\nabla y_{i}\\left(t\\right)}.$$\n",
    "\n",
    "### 2.4 Evaluation Trace and Computational Graph\n",
    "Our approach for automated differentiation is founded on the concepts of evaluation trail and computational graph.\n",
    "\n",
    "The evaluation trace traces each operation layer while the input function and its derivative are evaluated. The evaluation trace contains the traces, elementary operations, numeric values, elementary derivatives, and partial derivatives at each step.\n",
    "\n",
    "The computational graph represents the evaluation path graphically. It contains the traces and elementary operations of all the stages, linking them with arrows leading from the input to the output of each step, allowing us to better comprehend the function's structure and evaluation trail. In forward mode, operations are executed from the beginning to the conclusion of a graph or evaluation trace. Reverse mode executes the processes in reverse, applying the chain rule each time to get the trace's derivative.\n",
    "For example, we have a function:\n",
    "$$f(x,y, z) = x*y + z$$\n",
    "![image1.png](image1.png)\n",
    "\n",
    "### 2.5 Reverse Mode\n",
    "The idea in reverse mode AD is that the gradients would be propagated backwards or ‘in reverse ’ from the output leveraging a generalized backpropagation algorithm. This is achieved by complementing each of the input variable v with an adjoint defined as `v=∂y/∂v`.\n",
    "\n",
    "The reverse mode AD is a two-pass process as compared to an m-pass forward mode. The derivatives are computed in the second phase of this two phase process. \n",
    "\n",
    "- During the first phase, the original function is run forward populating intermediate variables `vi` and maintaining the dependencies in the computational graph. \n",
    "\n",
    "- In the second phase, we calculate the derivatives by propagating adjoints - from outputs to the inputs, that is, in reverse.\n",
    "\n",
    "* We first thought of implementing higher order derivatives as an extension of our project but since, we realize that in neural networks, reverse mode plays an essential role, we decided to change our extension to implementing reverse mode. This feature has vital contribution in the world of Machine learning that can be further used to solve problems from various fields including health care, telecommunications, finance and marketing etc. We also discussed this change in approach with our assigned TF, and after his approval we implemented the change.\n",
    "\n",
    "### 2.6 Forward vs Reverse Mode\n",
    "When comparing the modes of operation, there are two major factors to take into account:\n",
    "\n",
    "1. Computation Time and Storage:\n",
    "    - We are required to store the derivatives in the forward mode.We only need to store activations in the case of reverse mode. \n",
    "    - While the derivative is computed in forward mode AD concurrently with the variable evaluation, the reverse mode requires a separate backward phase.\n",
    "    \n",
    "    \n",
    "2. Input & Output Dimensions\n",
    "     - The output dimension is much greater than the input dimension (n<<m) : Compared to the reverse mode, forward mode is computationally less expensive. \n",
    "     - The output dimension is much smaller than the input dimension(n>>m): Deep Learning/Artificial neural networks have this property. As compared to the reverse mode, forward mode is computationally less expensive. \n",
    "\n",
    "\n",
    "## 3. How To Use `fab_AD`?\n",
    "\n",
    "### 3.1 Package Installation, testing and import\n",
    "#### 3.1.1 Installation from the source\n",
    "Our package is for Python 3 only. You can access our package by cloning our repository. \n",
    "\n",
    "- To clone run our repository, run `git clone git@code.harvard.edu:CS107/team32.git` from command line. \n",
    "- Once you clone the repository you can use `cd team32` where you can find all the files. \n",
    "- From there, you can use `cd src` to go where all the modules reside. \n",
    "- Then use `python -m pip install toml` which will install all the requirements specified in toml.\n",
    "```bash\n",
    "git clone git@code.harvard.edu:CS107/team32.git\n",
    "cd team32\n",
    "cd src\n",
    "python -m pip install toml\n",
    "```\n",
    "\n",
    "#### 3.1.2 Installation from internet\n",
    "\n",
    "fab_AD is available at (https://test.pypi.org/simple/ fab-ad). You can download it by the command given below.\n",
    "```bash\n",
    "pip install -i https://test.pypi.org/simple/ fab-ad\n",
    "```\n",
    "\n",
    "#### 3.1.3 Development Environment setup\n",
    "1. Create virtual environment\n",
    "\n",
    "2. Use poetry to install Dependencies\n",
    "```bash\n",
    "python3 -m pip install poetry\n",
    "poetry install\n",
    "```\n",
    "\n",
    "\n",
    "### 3.2 User Guide\n",
    "\n",
    "#### 3.2.1 Import functions\n",
    "\n",
    "Once you install the package, you can import relevant modules by running.\n",
    "```python\n",
    "from fab_ad.fab_ad_tensor import FabTensor, AdMode\n",
    "from fab_ad.fab_ad_session import fab_ad_session\n",
    "from fab_ad.fab_ad_diff import auto_diff\n",
    "```\n",
    "\n",
    "#### 3.2.2 Programming Usage\n",
    "\n",
    "After importing the functions, you could initiate the FabTensor object by giving the point where you wish to differentiate. FabTensor can take in a vector input values, representing a point's coordinates in multi-dimensional space. Moreover, you could also add other supplementary features as in the code demo provided below.\n",
    "\n",
    "\n",
    "#### Usage: Forward Mode AD\n",
    "\n",
    "```python \n",
    "from fab_ad.fab_ad_tensor import FabTensor, AdMode\n",
    "from fab_ad.fab_ad_session import fab_ad_session\n",
    "from fab_ad.fab_ad_diff import auto_diff\n",
    "from fab_ad.constants import *\n",
    "\n",
    "# multiple scalar input; single scalar output; forward ad\n",
    "\n",
    "# initialize the fab_ad session with number of input variables. if unsure, set num_inputs to a high number (defaults to 10)\n",
    "fab_ad_session.initialize()\n",
    "\n",
    "# define the input variables\n",
    "x = FabTensor(value=3, identifier=\"x\")\n",
    "y = FabTensor(value=-4, identifier=\"y\")\n",
    "\n",
    "# compute the output variable\n",
    "z = x ** 2 + 2 * y ** 2\n",
    "\n",
    "# compute the gradient of the output variable with respect to the input variables\n",
    "result = auto_diff(z, mode=AdMode.FORWARD)\n",
    "\n",
    "assert result.value == 41\n",
    "assert all(result.gradient == np.array([6, -16]))\n",
    "print(result)\n",
    "# Function 0: Value: 41\n",
    "# Gradient w.r.t x = 6.0\n",
    "# Gradient w.r.t y = -16.0\n",
    "```\n",
    "\n",
    "#### Usage: Reverse Mode AD\n",
    "\n",
    "```python\n",
    "from fab_ad.fab_ad_tensor import FabTensor, AdMode\n",
    "from fab_ad.fab_ad_session import fab_ad_session\n",
    "from fab_ad.fab_ad_diff import auto_diff\n",
    "from fab_ad.constants import *\n",
    "\n",
    "# Multiple scalar input; scalar output; reverse ad\n",
    "# initialize fab_ad session with number of input variables. if unsure, set num_inputs to a high number\n",
    "fab_ad_session.initialize()\n",
    "# initialize input variables\n",
    "x = FabTensor(value=3, identifier=\"x\")\n",
    "y = FabTensor(value=-4, identifier=\"y\")\n",
    "# compute output variable\n",
    "z = x ** 2 + 2 * y ** 2\n",
    "# compute gradient of output variable with respect to input variables via reverse mode AD\n",
    "result = auto_diff(z, mode=AdMode.REVERSE)\n",
    "\n",
    "assert result.value == 41\n",
    "assert all(result.gradient == np.array([6, -16]))\n",
    "print(result)\n",
    "# Function 0: Value: 41\n",
    "# Gradient w.r.t x = 6.0\n",
    "# Gradient w.r.t y = -16.0\n",
    "```\n",
    "\n",
    "#### Usage: Gradient Descent\n",
    "\n",
    "```python\n",
    "from fab_ad.fab_ad_tensor import FabTensor, AdMode\n",
    "from fab_ad.fab_ad_session import fab_ad_session\n",
    "from fab_ad.fab_ad_diff import auto_diff\n",
    "from fab_ad.constants import *\n",
    "\n",
    "def function_derivative(x: FabTensor, y: FabTensor):\n",
    "    # compute output variable\n",
    "    z = x**2 + y**4\n",
    "    # compute gradient of output variable with respect to input variables\n",
    "    return auto_diff(output=z, mode=AdMode.FORWARD).gradient\n",
    "\n",
    "def gradient_descent(\n",
    "    function_derivative, start, learn_rate, n_iter=10000, tolerance=1e-10\n",
    "):\n",
    "    # initialize the vector\n",
    "    vector = start\n",
    "    # initialize the fab_ad session with number of input variables. if unsure, set num_inputs to a high number\n",
    "    fab_ad_session.initialize()\n",
    "    # initialize the input variables\n",
    "    x = FabTensor(value=vector[0], identifier=\"x\")\n",
    "    y = FabTensor(value=vector[1], identifier=\"y\")\n",
    "    for i in range(n_iter):\n",
    "        # compute the gradient descent step\n",
    "        diff = -learn_rate * function_derivative(x, y)\n",
    "        if np.all(np.abs(diff) <= tolerance):\n",
    "            break\n",
    "        # update the vector\n",
    "        vector += diff\n",
    "        # update the input variables\n",
    "        x += diff[0]\n",
    "        y += diff[1]\n",
    "        \n",
    "        if (i%1000) == 0:\n",
    "            print(f\"iteration {i}: {vector}\")\n",
    "    \n",
    "    return vector\n",
    "\n",
    "\n",
    "start = np.array([1.0, 1.0])\n",
    "print(gradient_descent(function_derivative, start, 0.2, tolerance=1e-08).round(4))\n",
    "\n",
    "# iteration 0: [0.6 0.2]\n",
    "# iteration 1000: [8.49966157e-223 2.47685235e-002]\n",
    "# iteration 2000: [4.9406565e-324 1.7593023e-002]\n",
    "# iteration 3000: [4.94065646e-324 1.43868515e-002]\n",
    "# iteration 4000: [4.94065646e-324 1.24691641e-002]\n",
    "# iteration 5000: [4.9406565e-324 1.1158074e-002]\n",
    "# iteration 6000: [4.94065646e-324 1.01891453e-002]\n",
    "# iteration 7000: [4.94065646e-324 9.43548979e-003]\n",
    "# iteration 8000: [4.94065646e-324 8.82762731e-003]\n",
    "# iteration 9000: [4.94065646e-324 8.32389824e-003]\n",
    "# [0.     0.0079]\n",
    "```\n",
    "\n",
    "#### Usage: Newton-Raphson\n",
    "\n",
    "```python\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from fab_ad.constants import *\n",
    "from fab_ad.fab_ad_tensor import FabTensor, AdMode\n",
    "from fab_ad.fab_ad_session import fab_ad_session\n",
    "from fab_ad.fab_ad_diff import auto_diff\n",
    "\n",
    "# Function to find the root\n",
    "def newtonRaphson(x):\n",
    "    \n",
    "    def func(x):\n",
    "        # compute output variable and return value\n",
    "        z = x * x * x - x * x + 2\n",
    "        return auto_diff(output=z, mode=AdMode.FORWARD).value\n",
    "    \n",
    "    def derivFunc(x):\n",
    "        # compute output variable and return gradient\n",
    "        z = x * x * x - x * x + 2\n",
    "        return auto_diff(output=z, mode=AdMode.FORWARD).gradient\n",
    "    \n",
    "    # initialize the fab_ad session with number of input variables. if unsure, set num_inputs to a high number\n",
    "    fab_ad_session.initialize()\n",
    "    tensor = FabTensor(value=x, identifier=\"x\")\n",
    "    h = func(tensor) / derivFunc(tensor)\n",
    "    while True:\n",
    "        if isinstance(h, float):\n",
    "            if abs(h) < 0.0001:\n",
    "                break\n",
    "        else:\n",
    "            if max(abs(h)) < 0.0001:\n",
    "                break\n",
    "        # x(i+1) = x(i) - f(x) / f'(x)\n",
    "        x = x - h\n",
    "        tensor = tensor - h\n",
    "        h = func(tensor) / derivFunc(tensor)\n",
    "    print(\"The value of the root is : \", x)\n",
    "\n",
    "\n",
    "# Driver program to test above\n",
    "x0 = -20.00 # Initial values assumed\n",
    "newtonRaphson(x0)\n",
    "# The value of the root is :  -1.0000001181322415\n",
    "\n",
    "x0 = [-10.00, 10.00] # Initial values assumed\n",
    "newtonRaphson(x0)\n",
    "# The value of the root is :  [-1.         -1.00000001]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 3.2 Interface Usage\n",
    "\n",
    "For the sake of simplicity, we have incorporated a simple yet efficient user interface for our project. Our user interface is quite self explanatory where the code snippets are already present. Users can run these basic functionalities of our project by just clicking on the 'Run' button given on the side of the window. After running the code, users can see the output that our code generates. The link to our user interface is given below:\n",
    "\n",
    "(https://fab-ad.streamlit.app/)\n",
    "\n",
    "## 4. Software Organization\n",
    "\n",
    "The home directory of our software package would be structured as follows.\n",
    "\n",
    "```bash\n",
    "- LICENSE.txt\n",
    "- README.md\n",
    "- pyproject.toml\n",
    "- docs/\n",
    "    * README.md\n",
    "    * milestone1.ipynb\n",
    "    * milestone2_progress.md\n",
    "    * milestone2.ipynb\n",
    "    * documentation.ipynb\n",
    "    * documentation.md\n",
    "    * api\n",
    "- setup.py\n",
    "- demo.ipynb\n",
    "- src/\n",
    "    - fab_ad/\n",
    "        * constants.py\n",
    "        * fab_ad.py\n",
    "        * fab_admath.py\n",
    "\n",
    "- tests/\n",
    "    - fab_ad/\n",
    "        * test_fab_ad.py\n",
    "        * test_fab_admath.py\n",
    "\n",
    "- .github/workflows\n",
    "    - test.yml\n",
    "    - coverage.yml\n",
    "```\n",
    "Specificly speaking, the README file would contain a general package description and the necessary information for users to navigate in the subdirectories. The `docs` directory include our previous milestones documentation, and would further our documentation and testing api.\n",
    "\n",
    "Moreover, to package our model with PyPI, we would include `setup.py` and `fab_ad` directory in `src` directory, where  the source code of our package would be stored. These core modules include: `fab_ad.py`, `fab_admath.py` and `constants.py` . Furthermore, we would put a collection of test cases in `tests` directory. The tests are run through Pytest. Last but not least, we would include `test.yml` and `coverage.yml` in our home directory for integrated test. In addition, we would also include a simple tutorial `demo.ipynb` for demo of our code in the home directory.\n",
    "\n",
    "To distribute our package, we would use PyPI so that users could easily install the package with pip install.\n",
    "\n",
    "After installing the package, users can use `from fab_ad import FabTensor` to import the package. These two modules are where the core of this package resides:\n",
    "\n",
    "- `fab_ad`: defines the fab_ad object class that we use to perform automatic differentiation and overwrites basic math operation dunder methods for AD. \n",
    "\n",
    "- `fab_admath`: defines functions that perform elementary math operations on AD, which include those that cannot be performed by overwriting dunder methods, such as logarithm and trigonometry.\n",
    "\n",
    "\n",
    "## 5. Implementation Details\n",
    "\n",
    "Details about the implementation including usage and purpose of each class and method was generated using sphinx and is hosted on GitHub pages. Current implementation of our automatic differentiation library involves the first derivative of a scalar/vector function w.r.t each independent variable/seed vector or a linear combination of the seed vectors.\n",
    "\n",
    "The core modules in the implementation are `fab_ad` package are `fab_ad_session`, `fab_ad_math`, `fab_ad_diff`, and `fab_ad_tensor`. `fab_ad_tensor` module has `FabTensor` class which represents a variable/node in the graph (although we don't explicitly store the graph). The attributes `value`, `derivative`, and `identifier` represent the value (primal trace), gradient (tangent trace) in forward mode, and a unique identifier string for the tensor respectively. Another attribute called `depth` represents level in the graph where `depth = 0` represents the independent input variables. `source` attribute has the list of nodes from the previous layer used to compute the current node. `_reverse_mode_gradient` is used to store gradients in reverse mode. Before running reverse mode we use `zero_grad` method (inspired from `PyTorch`) to set reverse mode gradient to `0`. `FabTensor` class also has `getter` and `setter` methods to fetch and update the reverse mode gradient. All the basic elementary and mathematical operators are supported by overriding dunder methods to compute the value and gradients in both modes. `fab_ad_session` module contains the `fab_ad_session` object (instance of `FabAdSession()` class), used to initialize the number of maximum number of independent variables. This is essential to keep track of the variables w.r.t which the gradient of output variable needs to be computed. `fab_ad_math` module contains the more complex mathematical functions that are overridden for computing the value and gradients in both modes as each function is called. Example functions are trigonometric, inverse trigonometric, hyperbolic, exponential, logistic, logarithm, square root, etc. `fab_ad_diff` module has functions `auto_diff` to compute gradient of output variables. `auto_diff` method is a wrapper function which calls one of `forward_mode_gradient`, and `reverse_mode_gradient` depending on the mode argument (or based on a heuristic in case `mode` is not passed).\n",
    "\n",
    "<!-- Value is the primal trace, derivative is an array which contains first order derivative w.r.t each seed vector, and identifier is the function expression. Each step of evaluation of the derivative/value for the function expression results in a new FabTensor object which is computed and initialized in the implemented methods for each mathematical operation. fab_ad module also contains basic overridden inequality methods, len method that returns the number of independent variables, five basic overridden methods for elementary mathematical operations - add, subtract, multiply, divide, and power, and the directional_derivative method which computes the derivative w.r.t a given seed vector. fab_admath module contains other more complex mathematical operators and functions such as sigmoid, sin, arcsin, sqrt, etc.\n",
    "\n",
    "For the next milestone we plan on adding a few more math functions - trigonometric (cosec, sec, cot) and hyperbolic. Also, the current implementation only computes the first order derivative for a scalar function. As part of the extension for this project we plan on implementing higher order and mixed derivatives of vector functions. Specifically we will implement the Hessian matrix which includes the derivative of a function twice w.r.t each independent variable (similar to the Laplacian operator) and mixed derivatives (i.e. derivative w.r.t one independent variable followed by another independent variable). -->\n",
    "\n",
    "## 6. Broader Impact and Inclusivity Statement\n",
    "\n",
    "### 6.1 Broader Impact\n",
    "\n",
    "We hope that our package will be used in a variety of disciplines, including physics, engineering, applied mathematics, astronomy, and even domains that the package's creators could never have envisaged. We believe that this package can be used to perform automatic differentiations accurately and effectively as well as serve as a model for the future creation of improved automatic differentiation packages. We see several opportunities for this package to be improved and would be pleased to see them realized.\n",
    "\n",
    "On the other hand, we don't want to see this program being used as a shortcut for differentiating work or for plagiarism or cheating. This package's open-source nature makes it available to everyone, but it also leaves it vulnerable to those who intend to use it for plagiarism. Users should be mindful of this nature and pick their method of using this package carefully. This software is not intended to be used as a shortcut for differentiation procedures. It could be used to check results for derivative calculations done manually or using other algorithms, although derivative calculation techniques should still be used instead. These exercises serve a purpose, and using this software to find the solutions does not advance learning.\n",
    "\n",
    "Also evident is the connection between our automatic differentiation algorithms and mathematical concepts like the Leibniz Rule and the Faa di Bruno Formula that we made while working on this project. It shouldn't be the first time that these formulas have been employed to compute higher-order derivatives, but it provided motivation for us to carry out the implementation ourselves. In order to close the gap between theories and applications, we want for our project to serve as a case study. This experience demonstrates that now is the perfect time for all types of information to come together in order to support new discoveries, which will enable us and many students to continue working toward this aim.\n",
    "\n",
    "### 6.2 Inclusivity Statement\n",
    "\n",
    "Users and collaborators from all origins and identities are welcome to the fab_ad package and its developer. As was seen during the creation of this package, we think that trust, respect, and care are the foundations of greatness in a collaborative endeavor. In an effort to reach as many people as possible who are interested in this package, we made every effort to make it as inclusive and user-friendly as possible by including the necessary documentation and instructions. Although this package was created using Python and English, anyone who is proficient in another language or programming language is welcome to contribute. Pull requests are examined and approved by all developers while this package is being developed.\n",
    "\n",
    "Every time one of us feels the need to start a pull request, that person would talk to the other members and come to a consensus. We would adore the opportunity to carry this constructive dialogue into other collaborations with this package.\n",
    "\n",
    "\n",
    " \n",
    "## 7. Future Direction\n",
    "\n",
    "We are implementing the reverse mode as part of this project's extension. Numerous optimization issues can be resolved via automatic differentiation. One forward traversal of a computation graph is used in reverse mode to set up the trace. The function's whole gradient is then calculated in a single traversal of the graph in the reverse direction. This approach is significantly more effective for problems with lots of variables. This addition to our program is absolutely necessary because, in the real world, we frequently address problems involving numerous variables and functions.\n",
    "\n",
    "The reverse mode Automatic differentiation works best for scalar functions with many variables. Moreover, the reverse mode gradient computation is also one of the revolutionary developments in neural networks. We believe that implementing significant neural structures can be done effectively using our package. One of the key components of machine learning and deep learning is neural networks. In practically every significant field, these neural networks can be employed for prediction and optimization. Our package is able to cope with the hidden layers of a neural network, which are frequently built as matrices, because the reverse mode functionality enables our package to carry out the automatic differentiation for matrix. Our package can also help to visualize the entire back propagation process in neural networks, which will undoubtedly aid in improving understanding of this process.\n",
    "\n",
    "Furthermore, As part of the extension for this project we plan on implementing higher order and mixed derivatives of vector functions. Specifically we will implement the Hessian matrix which includes the derivative of a function twice w.r.t each independent variable (similar to the Laplacian operator) and mixed derivatives (i.e. derivative w.r.t one independent variable followed by another independent variable). Currently, we have implemented only the first derivative of a function w.r.t all the seed vectors/independent variables.\n",
    "\n",
    "Currently the derivative data member of FabTensor object is a 1D array. This would change to 2D array to accommodate the Hessian matrix. Also, we would have to update each of our elementary math functions to return the second derivatives by taking the derivative of the first derivative. The main challenge here will be to efficiently compute the second derivatives from the first derivatives by reusing most of the code already written and using a helper function to call the same derivative function twice.\n",
    "\n",
    "Directory structure would remain the same. Modules and classes would also remain the same because we don't need a new class to implement the second derivative. We would need a new method in the fab_ad class to call the derivative for the second time (derivative of the first derivative). Also, we would have to update each of our elementary math functions to return the second derivatives. Data structure of the derivative member of the FabTensor class would change from being a 1D array to 2D array to store all the second derivatives i.e. the Hessian matrix.\n",
    "\n",
    "Higher order and mixed derivatives, specifically second order derivatives can then be used to determine the concavity, convexity, the points of inflection, and local extrema of functions. As pointed out earlier it can also be used to compute the Laplacian operator which is useful in describing many different phenomena, from electric potentials, to the diffusion equation for heat and fluid flow, and quantum mechanics.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
